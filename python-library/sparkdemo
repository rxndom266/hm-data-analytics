import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, sha1
spark = SparkSession.builder.getOrCreate()

def cleanData(df):
    cdf = df.where(df.person_id.isNotNull() & df.first_name.isNotNull() & df.last_name.isNotNull() & df.address.isNotNull() & df.pincode.isNotNull())
    cdf = cdf.filter(cdf.pincode.between(10000, 99999))
    return cdf
    
def masterData(df):
    mdf = df.withColumn("mastered_person_id", sha1(concat(df['first_name'],df['last_name'],df['address'],df['pincode'])))
    return mdf
